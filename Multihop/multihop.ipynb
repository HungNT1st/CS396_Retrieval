{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihop Retrieval System\n",
    "\n",
    "This notebook implements a multi-hop retrieval system that:\n",
    "1. Uses a pre-populated vector database (FAISS)\n",
    "2. Decomposes complex questions into sub-questions using an LLM\n",
    "3. Retrieves relevant documents for each sub-question\n",
    "4. Answers each sub-question based on retrieved documents\n",
    "5. Combines the sub-answers into a final comprehensive answer and answer the big question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_database(dataset_path='dataset/corpus.json', save_path=\"faiss_index_with_metadata\"):\n",
    "    # Initialize embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create document objects\n",
    "    docs = []\n",
    "    for item in data:\n",
    "        title = item.get('title', 'Unknown Title')  \n",
    "        body = item.get('body', '')\n",
    "        doc = Document(page_content=body, metadata={\"title\": title})\n",
    "        docs.append(doc)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Store the embeddings\n",
    "    vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "    vector_store.save_local(save_path)\n",
    "    \n",
    "    print(f\"FAISS index with metadata stored successfully at {save_path}!\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_database(load_path=\"faiss_index_with_metadata\", preview=False):\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vector_store = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    if preview:\n",
    "        print(\"Displaying first 5 indexed documents:\\n\")\n",
    "        for i, doc in enumerate(vector_store.docstore._dict.values()):\n",
    "            if i >= 5:  \n",
    "                break\n",
    "            print(f\"Index {i + 1}:\")\n",
    "            print(f\"Title: {doc.metadata.get('title', 'No Title')}\")\n",
    "            print(f\"Content: {doc.page_content[:300]}...\") \n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_question(question, model_name=\"gpt-4o\", temperature=0):\n",
    "    # Create a prompt template\n",
    "    decompose_prompt = PromptTemplate(\n",
    "        template=\"You are given a question. If the question can be broken into multiple smaller phrases, \"\n",
    "                 \"return sub phrases and the question. However, if the question is simple and cannot be \"\n",
    "                 \"broken into smaller phrases, just return the original question. A sub phrase is just a \"\n",
    "                 \"relevant part of the question, not a whole new question. \\nQuestion: {question}\\nSub-questions:\",\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize the model\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "    decompose_chain = decompose_prompt | llm\n",
    "    \n",
    "    # Run the chain to get sub-questions\n",
    "    sub_questions_text = decompose_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Parse the sub-questions\n",
    "    sub_questions = [sq.strip().lstrip(\"0123456789. \") for sq in sub_questions_text.content.splitlines() if sq.strip()]\n",
    "    \n",
    "    print(f\"Decomposed into {len(sub_questions)} sub-questions:\")\n",
    "    for i, sq in enumerate(sub_questions, 1):\n",
    "        print(f\"{i}. {sq}\")\n",
    "    \n",
    "    return sub_questions, llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval and Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_answer_subquestions(sub_questions, vector_store, k=3):\n",
    "    sub_answers = {}\n",
    "    \n",
    "    for i, sub_q in enumerate(sub_questions, start=1):\n",
    "        print(f\"\\nProcessing sub-question {i}: {sub_q}\")\n",
    "        \n",
    "        # Retrieve top k relevant documents\n",
    "        docs_result = vector_store.similarity_search(sub_q, k=k)  \n",
    "        \n",
    "        if not docs_result:\n",
    "            print(\"No relevant documents found for this sub-question.\")\n",
    "            sub_answers[sub_q] = \"No relevant information found.\"\n",
    "            continue  \n",
    "\n",
    "        top_doc = docs_result[0]\n",
    "        content = top_doc.page_content\n",
    "        \n",
    "        print(f\"Top document title: {top_doc.metadata.get('title', 'Unknown')}\")\n",
    "        print(f\"Top document snippet: {content[:200]}...\")\n",
    "        \n",
    "        # Extract the first sentence as a simple answer\n",
    "        answer_piece = content.split('.')[0].strip()\n",
    "        sub_answers[sub_q] = answer_piece\n",
    "        \n",
    "        print(f\"Extracted answer: {answer_piece}\")\n",
    "    \n",
    "    return sub_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(sub_answers, llm, original_question):\n",
    "    # Format the sub-answers for the prompt\n",
    "    formatted_answers = \"\\n\".join([f\"Sub-question: {q}\\nAnswer: {a}\" for q, a in sub_answers.items()])\n",
    "    \n",
    "    # Create the final prompt template with 10-word limit\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=(\n",
    "            \"You are answering a complex question based on retrieved sub-answers. \\n\\n\"\n",
    "            \"Original question: {original_question}\\n\\n\"\n",
    "            \"Below are the sub-questions and their extracted answers that provide context:\\n\\n\"\n",
    "            \"{formatted_answers}\\n\\n\"\n",
    "            \"Using all the sub-answers as context, answer the original question in EXACTLY 10 words or less.\"\n",
    "        ),\n",
    "        input_variables=[\"original_question\", \"formatted_answers\"]\n",
    "    )\n",
    "    \n",
    "    # Generate the final response\n",
    "    final_response = llm.invoke(\n",
    "        final_prompt_template.format(\n",
    "            original_question=original_question,\n",
    "            formatted_answers=formatted_answers\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return final_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Multi-hop Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihop_retrieval(question, create_new_db=False):\n",
    "    # Create or load vector database\n",
    "    if create_new_db:\n",
    "        vector_store = create_vector_database()\n",
    "    else:\n",
    "        vector_store = load_vector_database()\n",
    "    \n",
    "    # Decompose the question\n",
    "    sub_questions, llm = decompose_question(question)\n",
    "    \n",
    "    # Retrieve documents and extract answers for each sub-question\n",
    "    sub_answers = retrieve_and_answer_subquestions(sub_questions, vector_store)\n",
    "    \n",
    "    # Generate the final answer\n",
    "    final_answer = generate_final_answer(sub_answers, llm, question)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL ANSWER:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(final_answer)\n",
    "    \n",
    "    return {\n",
    "        \"original_question\": question,\n",
    "        \"sub_questions\": sub_questions,\n",
    "        \"sub_answers\": sub_answers,\n",
    "        \"final_answer\": final_answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed into 5 sub-questions:\n",
      "1. Who is the individual associated with the cryptocurrency industry?\n",
      "2. Who is facing a criminal trial on fraud and conspiracy charges?\n",
      "3. Which publications reported on this individual?\n",
      "4. What is the individual accused of by prosecutors?\n",
      "5. What is the alleged motive behind the fraud?\n",
      "\n",
      "Processing sub-question 1: Who is the individual associated with the cryptocurrency industry?\n",
      "Top document title: Is Sam Bankman-Fried a bad ‘man’ or a good ‘boy’? Lawyers swap opening statements before first witnesses take the stand\n",
      "Top document snippet: The first was a victim: Marc-Antoine Julliard, a Paris-born cocoa trader who lives in London. In 2021, Julliard, who had coiffed hair and spoke with a strong French accent, decided to invest in crypto...\n",
      "Extracted answer: The first was a victim: Marc-Antoine Julliard, a Paris-born cocoa trader who lives in London\n",
      "\n",
      "Processing sub-question 2: Who is facing a criminal trial on fraud and conspiracy charges?\n",
      "Top document title: The FTX trial is bigger than Sam Bankman-Fried\n",
      "Top document snippet: allegedly used more than $100 million of customer funds to make political contributions; prosecutors can show evidence of those contributions in this trial, even though they aren’t part of the charges...\n",
      "Extracted answer: allegedly used more than $100 million of customer funds to make political contributions; prosecutors can show evidence of those contributions in this trial, even though they aren’t part of the charges brought\n",
      "\n",
      "Processing sub-question 3: Which publications reported on this individual?\n",
      "Top document title: The best shows to stream in December\n",
      "Top document snippet: headline-making pieces of investigative journalism: the five-year quest to unearth the truth behind rumours that the country’s most decorated living soldier, Ben Roberts-Smith, committed war crimes in...\n",
      "Extracted answer: headline-making pieces of investigative journalism: the five-year quest to unearth the truth behind rumours that the country’s most decorated living soldier, Ben Roberts-Smith, committed war crimes in Afghanistan\n",
      "\n",
      "Processing sub-question 4: What is the individual accused of by prosecutors?\n",
      "Top document title: The FTX trial is bigger than Sam Bankman-Fried\n",
      "Top document snippet: The first part of proving the government’s case is pretty simple and a little boring: prosecutors must show that certain transactions took place. Whatever records the Southern District of New York has...\n",
      "Extracted answer: The first part of proving the government’s case is pretty simple and a little boring: prosecutors must show that certain transactions took place\n",
      "\n",
      "Processing sub-question 5: What is the alleged motive behind the fraud?\n",
      "Top document title: In the end, the FTX trial was about the friends screwed along the way\n",
      "Top document snippet: Nishad Singh, a longtime family friend, who copped to what sounded an awful lot like classic embezzlement. He said he’d backdated transactions on FTX to help create doctored balance sheets to show to ...\n",
      "Extracted answer: Nishad Singh, a longtime family friend, who copped to what sounded an awful lot like classic embezzlement\n",
      "\n",
      "==================================================\n",
      "FINAL ANSWER:\n",
      "==================================================\n",
      "Sam Bankman-Fried, facing fraud trial, accused of personal gain.\n"
     ]
    }
   ],
   "source": [
    "# Example complex question\n",
    "complex_question = \"Who is the individual associated with the cryptocurrency industry facing a criminal trial on fraud and conspiracy charges, as reported by both The Verge and TechCrunch, and is accused by prosecutors of committing fraud for personal gain?\"\n",
    "\n",
    "# Run the multi-hop retrieval system\n",
    "result = multihop_retrieval(complex_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
